{
 "metadata": {
  "name": "ahassa11_PR2 _update"
 }, 
 "name": "ahassa11_PR2 _update", 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": "Programming Assignment #2 - 50 points\n=\nThis is the notebook for Programming Assignment #2.\n\nAssigned: June 26th, 2012\n\nDue: July 17th, 2012\n\n**Reminder** Every so often, restart the kernel, clear all output and run all code cells so you can be certain that you didn't\ndefine something out of order. However, you may, with this assignment, need to comment out previous\nwork so as to not drive yourself insane. Use triple quotes (\"\"\") to comment out large blocks of code.\n\nPlease read the program assignment description below **carefully**.\n\n**You should rename this notebook to be &lt;your JHED id>\\_PR2.ipynb** Do it right now.\n\n**Make certain the entire notebook executes before you submit it.** You can add as many code cells\nand text cells as you need to implement helper functions and show your results.\n\nYou should print out a hard copy of this notebook to hand in on the due date in class as well as\nsubmit the notebook itself through Sakai.\n\nChange the following variables:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "name = \"Amir Hassani\"\njhed_id = \"ahassa11\"\nif name == \"Student Name\" or jhed_id == \"sname1\":\n    raise Exception( \"Change the name and/or JHED ID...preferrably to yours.\")", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "markdown", 
     "source": "For this assignment, you're going to be looking at different aspects of reinforcement learning (RL).\nRL can take an unusually long time to run so for this assignment we're going to look at a small\nproblem from a number of different angles."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "from IPython.core.display import *\nfrom StringIO import StringIO\nimport operator", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 2
    }, 
    {
     "cell_type": "markdown", 
     "source": "The \"world\" variable below has a blank line at the start. In Python, triple quotes are used to define\na multiline string. This should look familiar...it's just a little bit smaller."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "world = \"\"\"\n.....**********\n.......********\n....xx*********\n....^xxx****~~*\n...^^xx**..~~~~\n.^^^xx^^....~~~\n.^^xx^^....^xxx\n..^^^^^......^x\n...^^^......^^x\n...~~~..^^^xxxx\n..~~~~~.^^xxx^.\n.~~~~~..^xx^...\n~~~~~..^^xx^.~~\n.~~~~..^**^....\n....x..****^^^^\n\"\"\"", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 3
    }, 
    {
     "cell_type": "markdown", 
     "source": "But I'm going to parse the world for you this time:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "io = StringIO( world)\nmap = []\nfor line in io.readlines():\n    line = line.strip()\n    if line == \"\": continue\n    map.append([x for x in line])\nprint map\nmap_limit = len( map) - 1\nprint map_limit", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "[['.', '.', '.', '.', '.', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*'], ['.', '.', '.', '.', '.', '.', '.', '*', '*', '*', '*', '*', '*', '*', '*'], ['.', '.', '.', '.', 'x', 'x', '*', '*', '*', '*', '*', '*', '*', '*', '*'], ['.', '.', '.', '.', '^', 'x', 'x', 'x', '*', '*', '*', '*', '~', '~', '*'], ['.', '.', '.', '^', '^', 'x', 'x', '*', '*', '.', '.', '~', '~', '~', '~'], ['.', '^', '^', '^', 'x', 'x', '^', '^', '.', '.', '.', '.', '~', '~', '~'], ['.', '^', '^', 'x', 'x', '^', '^', '.', '.', '.', '.', '^', 'x', 'x', 'x'], ['.', '.', '^', '^', '^', '^', '^', '.', '.', '.', '.', '.', '.', '^', 'x'], ['.', '.', '.', '^', '^', '^', '.', '.', '.', '.', '.', '.', '^', '^', 'x'], ['.', '.', '.', '~', '~', '~', '.', '.', '^', '^', '^', 'x', 'x', 'x', 'x'], ['.', '.', '~', '~', '~', '~', '~', '.', '^', '^', 'x', 'x', 'x', '^', '.'], ['.', '~', '~', '~', '~', '~', '.', '.', '^', 'x', 'x', '^', '.', '.', '.'], ['~', '~', '~', '~', '~', '.', '.', '^', '^', 'x', 'x', '^', '.', '~', '~'], ['.', '~', '~', '~', '~', '.', '.', '^', '*', '*', '^', '.', '.', '.', '.'], ['.', '.', '.', '.', 'x', '.', '.', '*', '*', '*', '*', '^', '^', '^', '^']]\n14"
      }
     ], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "markdown", 
     "source": "Next we create a map of movement costs:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "costs = { '.': 1, '*': 3, '^': 5, '~': 7}\ncosts", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 5, 
       "text": "{'*': 3, '.': 1, '^': 5, '~': 7}"
      }
     ], 
     "prompt_number": 5
    }, 
    {
     "cell_type": "markdown", 
     "source": "The goal is to reach (13,10) which has a reward of 100. However, there is also another spot, (7, 4)\nwith a reward of 25. (Keep the signs of costs and rewards straight in your implementation).\n\nThe movement directions are North (^), South (v), East (>) and West (&lt;). If you come to the edge of the map and attempt\nto move, you simply stay in the same square. If you try to move into a mountain (\"x\") then you stay in the same square.\n\nMovement is stochastic. 70% of the time you move in the intended direction (for example, if you attempt to move North, there\nwill be a 70% chance you actually move north). Otherwise, you move randomly in one of the other three directions with\nprobability 10%. You can use this information for Value Iteration (as the transition function) and for Q-Learning/SARSA\n(for the \"world simulator\").\n\nThe purpose of this assignment is to compare different approaches to reinforcement learning. You're\ngoing to implement a number of algorithms and compare their performance. The first thing you need\nto do is determine a good metric you can use. After you determine a good metric of comparison, you\nshould implement Value Iteration below.\n\nPick an appropriate value for gamma. You should run until error converges to 0.001. Make sure\nyour code collects data for your performance metric. (Why don't you need to run this experiment\nmore than once?) Write and use code to print out $\\pi^*$ below. Create as many code and text\ncells as you need to implement the algorithm and explain it."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "num_rows = len( map)\nnum_cols = len( map[0])\n\nv = []\n\nvalue_iter_actions = {'^': [ ( -1,  0), (  1,  0), (  0,  1), (  0, -1)], \n                      'v': [ (  1,  0), (  0,  1), (  0, -1), ( -1,  0)],\n                      '>': [ (  0,  1), (  0, -1), ( -1,  0), (  1,  0)],\n                      '<': [ (  0, -1), ( -1,  0), (  1,  0), (  0,  1)]\n              }\n\ntransitions = [ 0.7, 0.1, 0.1, 0.1]\n\nepsilon = 0.001\ngamma = 0.99\n\niteration_count = 0\n\nalpha = 0.25\nq = {}\nactions = [ ( -1,  0), (  1,  0), (  0,  1), (  0, -1)]\naction_translations = [ '^', 'v', '>', '<' ]", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 6
    }, 
    {
     "cell_type": "markdown", 
     "source": "Function to print the policy for Q-Learning and SARSA."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def print_policy ():\n    global num_cols, num_rows, q, actions\n    print \"%7s\" % \" \",\n    for j in range( 0, num_cols):\n        print \"%7i\" % j,\n    print\n    for r in range( 0, num_rows):\n        print \"%7i\" % r,\n        for c in range( 0, num_cols):\n            if map[ r][ c] != \"x\":\n                vals = []\n                for i in range(0, 4):\n                    ra = actions[i][0]\n                    ca = actions[i][1]\n                    r1 = r + ra \n                    c1 = c + ca\n                    if is_inbound( r1, c1):\n                        if ( r, c, ra, ca) in q.keys():\n                            vals.append( ( q[( r, c, ra, ca)], i))\n                        else:\n                            q[( r, c, ra, ca)] = 0\n                            vals.append( ( 0, i))\n                vals.sort(key=lambda tup: tup[0])    \n                print \"%7s\" % action_translations[ vals[ -1][ 1]],\n            else:\n                print \"%7s\" % \"x\",\n        print\n        \n    print \"%7s\" % \" \",\n    for j in range( 0, num_cols):\n        print \"%7i\" % j,\n    print\n    for r in range( 0, num_rows):\n        print \"%7i\" % r,\n        for c in range( 0, num_cols):\n            if map[ r][ c] != \"x\":\n                vals = []\n                for i in range(0, 4):\n                    ra = actions[i][0]\n                    ca = actions[i][1]\n                    r1 = r + ra \n                    c1 = c + ca\n                    if is_inbound( r1, c1):\n                        if ( r, c, ra, ca) in q.keys():\n                            vals.append( ( q[( r, c, ra, ca)], i))\n                        else:\n                            q[( r, c, ra, ca)] = 0\n                            vals.append( ( 0, i))\n                vals.sort(key=lambda tup: tup[0])    \n                print \"%7.2f\" % vals[-1][0], #action_translations[ vals[-1][1]],\n            else:\n                print \"%7s\" % \"x\",\n        print", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "markdown", 
     "source": "Function to print the policy for Value Iteration."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def print_value_iteration_policy ():\n    for j in range( 0, num_cols):\n        print \"%7i\" % j,\n    print\n    for i in range( 0, num_rows):\n        print i,\n        for j in range( 0, num_cols):\n            print \"%7.1f\" % (v[ i][ j]),\n        print\n        \n    print \"%7s\" % \" \",\n    for j in range( 0, num_cols):\n        print \"%7i\" % j,\n    print\n    for r in range( 0, num_rows):\n        print \"%7i\" % r,\n        for c in range( 0, num_cols):\n            if map[ r][ c] != \"x\":\n                utils = []\n                for direction in value_iter_actions:\n                    actions = value_iter_actions[ direction]\n                    sum = 0\n                    for i in range(0, 4):\n                        r1 = r + actions[ i][ 0]\n                        c1 = c + actions[ i][ 1]\n                        if is_inbound( r1, c1):\n                            sum += transitions[i] * v[ r1][ c1]\n                    utils.append( ( sum, direction))\n                utils.sort(key=lambda tup: tup[0])\n                print \"%7s\" % utils[-1][1],\n            else:\n                print \"%7s\" % \"x\",\n        print", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 8
    }, 
    {
     "cell_type": "markdown", 
     "source": "Returns sum of total rewards of path."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def total_reward ( path):\n    reward = 0\n    for node in path:\n        r = node[0]\n        c = node[1]\n        if r == 13 and c == 10:\n            reward += 100\n        elif r == 7 and c == 4:\n            reward += 25\n        else:\n            reward -= costs[ map[ r][ c]]\n    return reward", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 9
    }, 
    {
     "cell_type": "markdown", 
     "source": "Returns the path of the policy for SARSA and Q-Learning."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_path ():\n    global q, actions\n    r = 0\n    c = 0\n    path = []\n    path.append(( r, c))\n    items = {}\n    while True:\n        vals = []\n        for i in range(0, 4):\n            ra = actions[i][0]\n            ca = actions[i][1]\n            r1 = r + ra \n            c1 = c + ca\n            if is_inbound( r1, c1):\n                if ( r, c, ra, ca) in q.keys():\n                    vals.append( ( q[( r, c, ra, ca)], ( r, c, ra, ca)))\n                else:\n                    q[( r, c, ra, ca)] = 0\n                    vals.append( ( 0, ( r, c, ra, ca)))\n        vals.sort(key=lambda tup: tup[0])\n        r = r + vals[ -1][ 1][ 2]\n        c = c + vals[ -1][ 1][ 3]\n        path.append(( r, c))\n        \n        \n        if ( r, c) in items.keys():\n            print \"DUPLICATE!\"\n            items[( r, c)] = ( r, c)\n            return path\n        \n        items[( r, c)] = ( r, c)        \n        \n        if r == 13 and c == 10:\n            return path\n        elif r == 7 and c == 4:\n            return path\n    ", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 10
    }, 
    {
     "cell_type": "markdown", 
     "source": "Returns the path of the policy for Value Iteration."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_value_iteration_path ():\n    global q, actions\n    r = 0\n    c = 0\n    path = []\n    path.append(( r, c))\n    while True:\n        vals = []\n        for i in range(0, 4):\n            r1 = r + actions[ i][ 0]\n            c1 = c + actions[ i][ 1]\n            if is_inbound( r1, c1):\n                vals.append( ( v[ r1][ c1], ( r, c, r1, c1)))\n        vals.sort(key=lambda tup: tup[0])\n        r = vals[ -1][ 1][ 2]\n        c = vals[ -1][ 1][ 3]\n        path.append(( r, c))\n        if r == 13 and c == 10:\n            return path\n        elif r == 7 and c == 4:\n            return path", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 11
    }, 
    {
     "cell_type": "markdown", 
     "source": "Determine if row-column pair is within valid perimeters of map."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def is_inbound ( r1, c1):\n    global map, num_rows, num_cols\n    if ( r1 >= 0 and r1 < num_rows and c1 >= 0 and c1 < num_cols):\n        if( map[ r1][ c1] != 'x'):\n            return True\n    return False", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 12
    }, 
    {
     "cell_type": "markdown", 
     "source": "Return length of path generated from the policy for SARSA and Q-Learning.  This function\nrecognizes loops within the policy that prevent a goal state to be reached and returns 0."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_path_len ():\n    global q, actions\n    r = 0\n    c = 0\n    path = {}\n    path[( r, c)] = ( r, c)\n    while True:\n        vals = []\n        for i in range(0, 4):\n            ra = actions[i][0]\n            ca = actions[i][1]\n            r1 = r + ra \n            c1 = c + ca\n            \n            if is_inbound( r1, c1):\n                if ( r, c, ra, ca) in q.keys():\n                    vals.append( ( q[( r, c, ra, ca)], ( r, c, ra, ca)))\n                else:\n                    q[( r, c, ra, ca)] = 0\n                    vals.append( ( 0, ( r, c, ra, ca)))\n        vals.sort(key=lambda tup: tup[0])\n        r = r + vals[ -1][ 1][ 2]\n        c = c + vals[ -1][ 1][ 3]\n        if ( r, c ) in path.keys():\n            return 0\n        path[( r, c)] = ( r, c)\n        if r == 13 and c == 10:\n            return len( path.keys())\n        elif r == 7 and c == 4:\n            return len( path.keys())\n    ", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 13
    }, 
    {
     "cell_type": "markdown", 
     "source": "This function is used to select an alternate state from the intended state to mimic the stochastic nature of\nthe world."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_random_transition ( r, c, ra_not, ca_not):\n    global actions, q\n    while (True):\n        i = int( 4.0 * rand())\n        ra = actions[i][0]\n        ca = actions[i][1]\n        r1 = r + ra \n        c1 = c + ca\n        if is_inbound( r1, c1) and ra != ra_not and ca != ca_not:\n            if ( r, c, ra, ca) in q.keys():\n                return ( r, c, ra, ca)\n            else:\n                q[( r, c, ra, ca)] = 0\n                return ( r, c, ra, ca) ", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 14
    }, 
    {
     "cell_type": "markdown", 
     "source": "This function returns the action with the highest \"q\" value.  If there are more than one action that have the \nhighest \"q\" value then a random maximum \"q\" value is returned."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_max_action ( r, c):\n    global actions, q\n    vals = []\n    for i in range( 0, 4):\n        ra = actions[i][0]\n        ca = actions[i][1]\n        r1 = r + ra \n        c1 = c + ca\n        if is_inbound( r1, c1):\n            if ( r, c, ra, ca) in q.keys():\n                vals.append( ( q[( r, c, ra, ca)], ( r, c, ra, ca)))\n            else:\n                q[( r, c, ra, ca)] = 0\n                vals.append( ( 0, ( r, c, ra, ca)))\n    \n    vals.sort(key=lambda tup: tup[0])\n    del_list = []\n    \n    for i in range ( 0, len( vals)):\n        if vals[i][0] != vals[-1][0]:\n            del_list.append( i)\n                \n    del_list = sorted( del_list, reverse=True)\n    for i in del_list:\n        del vals[i]\n\n    return vals[ int( rand() * len( vals))][1]", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 15
    }, 
    {
     "cell_type": "markdown", 
     "source": "This function returns a random action from the specified state."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "def get_random_action ( r, c):\n    global actions, q\n    while (True):\n        i = int( 4.0 * rand())\n        ra = actions[i][0]\n        ca = actions[i][1]\n        r1 = r + ra \n        c1 = c + ca\n        if is_inbound( r1, c1):\n            if ( r, c, ra, ca) in q.keys():\n                return ( r, c, ra, ca)\n            else:\n                q[( r, c, ra, ca)] = 0\n                return ( r, c, ra, ca)", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 16
    }, 
    {
     "cell_type": "markdown", 
     "source": "Value Iteration algorithm."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "# Value Iteration\ndef Value_Iteration ():\n    global v, iteration_count\n    v = []\n    for r in range( 0, num_rows):\n        v.append( [0 for c in range( 0, num_cols)])\n\n    v1 = []\n    for r in range( 0, num_rows):\n        v1.append( [0 for c in range( 0, num_cols)])\n\n    max_diff = 0\n    iteration_count = 0\n    done = False\n    while done == False:\n        iteration_count += 1\n        max_diff = 0\n        for r in range( 0, num_rows):\n            for c in range( 0, num_cols):\n                v1[ r][ c] = 0\n        for r in range( 0, num_rows):\n            for c in range( 0, num_cols):\n                if map[ r][ c] != \"x\":\n                    utils = []\n                    for direction in value_iter_actions:\n                        actions = value_iter_actions[ direction]\n                        sum = 0\n                        for i in range(0, 4):\n                            r1 = r + actions[ i][ 0]\n                            c1 = c + actions[ i][ 1]\n                            if is_inbound( r1, c1):\n                                sum += transitions[i] * v[ r1][ c1]\n                        utils.append( sum)\n                    reward = -1 * costs[ map[ r][ c]] \n                    if r == 13 and c == 10:\n                        reward = 100\n                    elif r == 7 and c == 4:\n                        reward = 25\n                    v1[ r][ c] = reward + gamma * max( utils)\n        for i in range( 0, num_rows):\n            for j in range( 0, num_cols):\n                if map[ r][ c] != \"x\":\n                    if abs( v1[ i][ j] - v[ i][ j]) > max_diff:\n                        max_diff = abs( v1[ i][ j] - v[ i][ j])\n                    if v1[ i][ j] > v[ i][ j]:\n                        v[ i][ j] = v1[ i][ j]\n        if max_diff < 0.001:\n            done = True", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 17
    }, 
    {
     "cell_type": "markdown", 
     "source": "Now you're going to solve the same problem using Q-Learning. You can use any elaboration of the \nbasic Q-learning algorithm you see fit (for example, use batch update instead of incremental update).\nWhen learning the policy make sure you have a healthy balance of exploration and exploitation. When\nthe policy is learned, you should use only exploitation (greedy).\n\nYou\u2019ll need to pick appropriate values for gamma, epsilon (if you use soft max), \nalpha. Use the values above for your reward function.\n\nInstrument your code to collect data on your performance metric. You need to code up an implementation,\ngenerate $\\pi^*$ for one run and then run it multiple times and take the average\nresult of your performance metric. Make as many code and text cells as you need to implement and\nexplain your algorithm."
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "# Q-Learning\ndef QLearning ():\n    global q, iteration_count\n    iteration_count = 0\n    transition_count = 0\n    q = {}\n    avg_path_len = []\n    path_len = []\n    while( True):\n        r = 0\n        c = 0\n        terminal = False\n        while( terminal == False):\n            iteration_count += 1\n            if rand() > 0.33:\n                val = get_random_action( r, c)\n            else:\n                val = get_max_action( r, c)\n            \n            r1 = r + val[ 2]\n            c1 = c + val[ 3]\n\n            if float( transition_count) < float( iteration_count * 0.30):\n                transition_count += 1\n                val_transition = get_random_transition( r, c, val[ 2], val[ 3])\n                r1 = r + val_transition[ 2]\n                c1 = c + val_transition[ 3]\n            \n            qval = q[ val]\n        \n            val1 = get_max_action( r1, c1)\n            qval1 = q[ val1]\n        \n            reward = -1 * costs[ map[ r1][ c1]]\n            if r1 == 13 and c1 == 10:\n                reward = 100\n                terminal = True\n            elif r1 == 7 and c1 == 4:\n                reward = 25\n                terminal = True\n            \n            q[ val] = qval + alpha * ( reward + gamma * qval1 - qval)\n            r = r1\n            c = c1\n    \n        path_len.append( get_path_len() )\n        avg_path_len.append( mean( path_len))\n    \n        # test for convergence\n        if path_len[-1] != 0:\n            done = True\n            for i in range( -10, -1):\n                if int( avg_path_len[-1] * 100) != int( avg_path_len[i] * 100):\n                    done = False\n            if done == True:\n                break", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 18
    }, 
    {
     "cell_type": "markdown", 
     "source": "Now you should repeat the above experiment but implement SARSA instead. Use the same\nvalues of parameters and make sure to collect data on your performance metric.\n\nAgain, instrument your code to collect data on your performance metric. You need to code up an implementation,\ngenerate $\\pi^*$ for one run and then run it multiple times and take the average\nresult of your performance metric. Make as many code and text cells as you need to implement and\nexplain your algorithm."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "# SARSA\ndef SARSA ():\n    global q, iteration_count\n    q = {}\n    iteration_count = 0\n    transition_count = 0\n    avg_path_len = []\n    path_len = []\n    done = False\n    while( done == False):\n        r = 0\n        c = 0\n        terminal = False\n        if rand() > 0.33:\n            val = get_random_action( r, c)\n        else:\n            val = get_max_action( r, c)\n        while ( terminal == False):\n            iteration_count += 1\n            \n            r1 = r + val[ 2]\n            c1 = c + val[ 3]\n\n            if float( transition_count) < float( iteration_count * 0.30):\n                transition_count += 1\n                val_transition = get_random_transition( r, c, val[ 2], val[ 3])\n                r1 = r + val_transition[ 2]\n                c1 = c + val_transition[ 3]\n\n            if rand() > 0.33:\n                val1 = get_random_action( r1, c1)\n            else:\n                val1 = get_max_action( r1, c1)\n        \n            reward = -1 * costs[ map[ r1][ c1]]\n            if r1 == 13 and c1 == 10:\n                reward = 100\n                terminal = True\n            elif r1 == 7 and c1 == 4:\n                reward = 25\n                terminal = True\n\n            qval = q[ val]\n            qval1 = q[ val1]\n\n            q[ val] = qval + alpha * ( reward + gamma * qval1 - qval)\n \n            val = val1\n            r = r1\n            c = c1\n\n        # test for convergence\n        path_len.append( get_path_len() )\n        avg_path_len.append( mean( path_len))\n        if path_len[-1] != 0:\n            done = True\n            for i in range( -10, -1):\n                if int( avg_path_len[-1] * 100) != int( avg_path_len[i] * 100):\n                    done = False", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 19
    }, 
    {
     "cell_type": "markdown", 
     "source": "Run tests"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "Value_Iteration()\nprint \"Value Iteration done!\"\nprint get_value_iteration_path()\nprint_value_iteration_policy()\nv_result = []\nv_result.append( \"Value Iteration\")\nv_result.append( iteration_count)\nv_result.append( total_reward( get_value_iteration_path()))\nv_result.append( total_reward([ get_value_iteration_path()[-1]]))\nv_result.append( len( get_value_iteration_path()))\n\nQLearning()\nprint \"Q Learning done!\"\nprint get_path()\nprint_policy()\nq_result = []\nq_result.append( \"Q Learning\")\nq_result.append( iteration_count)\nq_result.append( total_reward( get_path()))\nq_result.append( total_reward([ get_path()[-1]]))\nq_result.append( len( get_path()))\n\n\nSARSA()\nprint \"SARSA done!\"\nprint get_path()\nprint_policy()\nsarsa_result = []\nsarsa_result.append( \"SARSA\")\nsarsa_result.append( iteration_count)\nsarsa_result.append( total_reward( get_path()))\nsarsa_result.append( total_reward([ get_path()[-1]]))\nsarsa_result.append( len( get_path()))\n\nresults = []\nresults.append( v_result)\nresults.append( q_result)\nresults.append( sarsa_result)\n    \nhtml = \"<table>\"\nhtml += \"<tr> <td></td> <td>Total Iterations</td> <td>Reward Path</td> <td>Ending Reward</td> <td>Path Length</td></tr>\"\nfor result in results:\n    html += \"<tr> <td>%s</td> <td>%s</td> <td>%s</td> <td>%s</td> <td>%s</td> </tr>\" % tuple( result)\nhtml += \"</table>\"\ndisplay_html( html, raw=True)", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "Value Iteration done!\n[(0, 0), (1, 0), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (9, 6), (10, 6), (11, 6), (12, 6), (12, 7), (13, 7), (13, 8), (13, 9), (13, 10)]\n      0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n0   106.5   134.9   137.0   129.0   116.4   131.7   155.8   172.1   185.8   196.1   196.0   189.1   179.5   164.6   127.3\n1   135.9   161.3   161.4   151.4   132.1   153.8   185.7   203.9   219.9   232.8   232.1   223.6   212.9   198.0   164.4\n2   147.4   172.5   170.1   148.3     0.0     0.0   149.1   193.0   232.3   247.6   246.0   236.4   225.6   211.4   177.4\n3   155.9   181.9   178.1   165.5   121.7     0.0     0.0     0.0   230.9   260.1   257.0   246.2   229.8   211.5   177.7\n4   164.4   191.6   184.5   165.3   121.6     0.0     0.0   212.4   267.2   274.5   267.7   251.6   231.9   208.6   168.1\n5   174.0   202.0   188.9   142.3     0.0     0.0   212.4   272.6   287.3   284.0   275.9   260.2   213.6   174.6   130.6\n6   188.6   219.6   204.3     0.0     0.0   217.4   274.8   298.9   298.4   291.6   280.9   242.1     0.0     0.0     0.0\n7   204.6   239.0   243.7   236.1   274.5   281.6   299.7   311.9   306.6   298.9   288.7   268.9   221.2   163.1     0.0\n8   215.1   251.0   261.8   270.0   286.1   300.5   315.6   322.2   313.9   303.0   283.8   242.0   199.4   149.3     0.0\n9   221.6   258.5   269.6   277.4   292.5   308.5   325.1   331.9   316.4   290.7   224.5     0.0     0.0     0.0     0.0\n10   223.7   260.7   271.5   286.1   301.9   318.0   334.1   342.7   318.4   244.4     0.0     0.0     0.0   290.9   256.5\n11   223.4   261.0   280.4   298.1   316.0   333.8   351.3   355.3   307.4     0.0     0.0   323.0   380.6   390.3   330.0\n12   215.7   260.3   283.2   303.9   325.6   349.6   362.2   371.0   350.7     0.0     0.0   418.9   448.8   421.6   354.4\n13   203.5   241.3   262.9   279.9   292.9   346.2   370.7   390.0   416.4   448.3   536.3   501.4   474.6   442.9   369.6\n14   160.6   204.2   222.7   215.0     0.0   270.2   315.7   333.9   357.0   388.1   449.4   426.8   402.8   370.4   288.2\n              0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n      0       v       v       v       v       v       >       v       v       v       v       v       v       v       v       <\n      1       >       v       v       <       >       >       >       >       >       v       v       v       v       <       <\n      2       >       v       v       <       x       x       >       >       >       v       v       v       <       <       <\n      3       >       v       v       <       <       x       x       x       v       v       v       <       <       <       <\n      4       >       v       <       <       <       x       x       v       v       v       v       <       <       <       <\n      5       >       v       v       <       x       x       v       v       v       v       < "
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "      <       <       <       <\n      6       >       v       v       x       x       v       v       v       v       v       <       <       x       x       x\n      7       >       v       v       >       v       v       v       v       v       <       <       <       <       <       x\n      8       >       >       >       >       >       >       v       v       <       <       <       <       <       <       x\n      9       >       >       >       >       >       >       v       v       <       <       <       x       x       x       x\n     10       >       >       >       >       >       >       v       v       <       <       x       x       x       v       v\n     11       >       >       >       >       >       >       v       v       <       x       x       v       v       v       <\n     12       >       >       >       >       >       >       >       v       v       x       x       v       v       <       <\n     13       >       >       ^       ^       >       >       >       >       >       >       >       <       <       <       <\n     14       >       ^       ^       ^       x       ^       ^       ^       ^       >       ^       ^       ^       ^       <\nQ Learning done!"
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "\n[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (9, 4), (9, 5), (9, 6), (9, 7), (10, 7), (11, 7), (12, 7), (13, 7), (13, 8), (13, 9), (13, 10)]\n              0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n      0       v       v       v       v       <       <       <       <       >       >       >       v       >       >       v\n      1       v       v       v       v       <       <       <       <       v       v       v       ^       >       ^       v\n      2       v       v       v       <       x       x       ^       >       v       v       v       v       >       >       <\n      3       v       v       v       <       <       x       x       x       v       v       v       v       <       ^       <\n      4       v       <       <       <       <       x       x       v       v       v       <       <       <       ^       v\n      5       v       <       <       <       x       x       v       v       v       v       v       <       <       >       ^\n      6       v       v       v       x       x       >       v       v       v       <       <       v       x       x       x\n      7       v       v       v       >       <       >       >       v       <       <       <       v       <       <       x\n      8       >       >       >       >       v       >       v       v       <       <       <       <       <       ^       x\n      9       >       v       >       >       >       >       >       v       v       <       <       x       x       x       x\n     10       >       >       v       >       >       v       >       v       v       <       x       x       x       v       v\n     11       ^       >       >       v       >       >       v       v       v       x       x       v       v       <       v\n     12       >       >       >       >       >       >       v       v       v       x       x       v       v       <       v\n     13       >       >       >       >       >       >       v       > "
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "      >       >       <       <       <       <       <\n     14       >       >       >       ^       x       >       >       >       >       ^       ^       <       <       <       ^\n              0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n      0   -4.03   -3.56   -4.40   -5.32   -6.72   -8.17  -11.13  -21.32  -21.96  -19.84  -18.66  -18.27  -18.60  -17.28  -16.96\n      1   -1.64   -1.87   -3.66   -4.51   -5.85   -9.41   -9.38  -17.72  -18.32  -18.09  -18.07  -18.62  -19.18  -18.23  -17.53\n      2    0.53   -0.51   -2.42   -4.02       x       x  -16.62  -18.35  -12.31  -12.29  -13.33  -17.29  -19.94  -18.33  -17.05\n      3    2.09    0.97   -1.16   -2.89   -7.63       x       x       x    5.30   11.22   -0.24  -14.49  -18.14  -19.66  -19.06\n      4    6.30    3.36    0.02   -3.40   -8.79       x       x   31.27   31.41   27.28    5.30   -3.27  -16.83  -22.16  -22.61\n      5    8.47    6.30    1.00   -4.02       x       x   33.18   40.15   40.46   39.16   20.22    5.75   -5.57  -21.00  -22.63\n      6   11.37   13.58   11.88       x       x   33.96   39.38   45.44   42.85   41.19   31.53   14.56       x       x       x\n      7   13.96   15.52   17.83   23.74    0.00   37.31   46.61   50.12   45.16   44.37   38.54   32.41   24.86    1.32       x\n      8   15.75   18.22   19.75   24.11   30.88   47.58   51.18   52.39   48.87   45.57   41.26   33.25   21.27   -2.75       x\n      9   15.73   17.85   21.46   31.21   39.73   49.42   52.75   55.29   53.58   45.60   40.18       x       x       x       x\n     10   15.31   17.56   26.50   36.19   47.46   56.28   55.80 "
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "  59.52   61.25   52.62       x       x       x   15.24    1.37\n     11   15.47   24.54   37.04   41.88   48.25   63.26   66.02   66.37   68.33       x       x   69.71   57.54   46.39    8.74\n     12   19.04   24.31   42.18   50.22   60.02   69.60   68.96   75.40   78.68       x       x   90.23   77.75   67.47   11.17\n     13   22.46   31.04   42.76   51.95   67.13   71.01   72.47   82.52   89.33   93.60    0.00   99.08   87.82   68.12   21.66\n     14   25.22   32.58   37.52   39.45       x   73.27   75.02   81.17   88.42   92.43   96.14   92.90   79.91   64.58    7.18\nSARSA done!"
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "\n[(0, 0), (0, 1), (1, 1), (2, 1), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (9, 1), (8, 1), (8, 2), (8, 3), (7, 3), (7, 4)]\n              0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n      0       >       v       <       <       <       <       >       >       >       <       v       >       >       <       v\n      1       v       v       <       <       <       <       >       >       >       v       v       v       >       >       v\n      2       v       <       v       ^       x       x       >       >       >       v       <       v       v       >       ^\n      3       v       v       <       ^       <       x       x       x       v       v       <       <       <       ^       v\n      4       v       <       <       <       <       x       x       v       >       v       <       v       <       <       v\n      5       v       v       <       <       x       x       v       v       v       v       <       <       <       >       ^\n      6       v       v       v       x       x       v       <       v       v       <       v       v       x       x       x\n      7       v       >       >       >       <       v       <       <       <       <       <       v       v       <       x\n      8       v       >       >       ^       ^       ^       <       v       <       <       <       ^       <       <       x\n      9       >       ^       ^       ^       ^       ^       ^       ^       <       ^       ^       x       x       x       x\n     10       ^       ^       <       ^       >       v       >       v       v       <       x       x       x       >       v\n     11       ^       ^       ^       >       >       v       v       v       v       x "
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "      x       v       >       >       <\n     12       ^       >       v       >       >       v       v       >       v       x       x       v       <       <       v\n     13       v       v       v       >       >       v       v       >       >       >       <       <       <       v       v\n     14       >       >       >       <       x       >       >       >       >       >       ^       ^       <       ^       ^\n              0       1       2       3       4       5       6       7       8       9      10      11      12      13      14\n      0  -87.34  -87.40  -87.48  -87.99  -88.34  -89.31  -88.60  -87.07  -82.47  -80.63  -79.28  -79.08  -78.95  -78.46  -78.17\n      1  -86.69  -87.16  -87.40  -87.85  -88.52  -89.54  -89.28  -84.31  -80.26  -78.37  -78.94  -78.85  -78.44  -77.50  -76.49\n      2  -85.98  -86.11  -87.29  -88.43       x       x  -87.63  -85.15  -76.55  -75.17  -76.44  -77.45  -79.28  -78.61  -77.99\n      3  -83.52  -84.86  -86.34  -89.26  -96.50       x       x       x  -67.64  -66.74  -72.03  -75.00  -77.17  -80.23  -81.33\n      4  -80.53  -85.17  -87.47  -90.71  -99.00       x       x  -58.01  -57.97  -57.98  -64.42  -65.59  -74.40  -80.51  -80.30\n      5  -73.98  -71.64  -79.52  -90.53       x       x  -51.87  -48.24  -52.55  -52.94  -59.63  -60.57  -70.62  -82.68  -79.31\n      6  -59.17  -62.34  -58.39       x       x  -26.80  -36.04  -46.53  -48.26 "
      }, 
      {
       "html": "<table><tr> <td></td> <td>Total Iterations</td> <td>Reward</td> <td>Ending Reward</td> <td>Path Length</td></tr><tr> <td>Value Iteration</td> <td>196</td> <td>39</td> <td>100</td> <td>24</td> </tr><tr> <td>Q Learning</td> <td>247155</td> <td>45</td> <td>100</td> <td>24</td> </tr><tr> <td>SARSA</td> <td>233518</td> <td>0</td> <td>25</td> <td>18</td> </tr></table>", 
       "output_type": "display_data"
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": " -51.16  -54.80  -54.59       x       x       x\n      7  -55.90  -52.51  -42.90    8.92    0.00   -9.38  -26.39  -38.74  -44.42  -51.50  -50.97  -52.91  -53.80  -55.20       x\n      8  -60.11  -46.30  -42.58  -24.76    3.31  -17.09  -36.42  -45.98  -46.89  -48.81  -52.15  -53.08  -53.42  -56.47       x\n      9  -61.59  -57.07  -45.19  -47.66  -31.14  -46.25  -36.98  -46.65  -47.33  -52.50  -53.73       x       x       x       x\n     10  -64.35  -65.17  -66.00  -69.66  -61.18  -53.20  -41.47  -28.55  -36.98  -47.73       x       x       x   -2.28   -4.03\n     11  -78.77  -77.21  -79.92  -72.44  -63.21  -49.28  -26.76  -24.15  -19.39       x       x    0.06   -1.13   -2.48   -0.87\n     12  -83.81  -85.92  -81.38  -75.69  -51.13  -33.25  -19.97  -12.71    9.49       x       x   27.19    4.52   -0.56   -3.70\n     13  -81.50  -81.17  -73.34  -54.22  -41.65  -14.98  -13.38   11.49   61.69   87.19    0.00   77.02   30.55   -3.24   -3.80\n     14  -80.09  -81.41  -78.86  -77.21       x  -12.09   -6.03    4.86   38.99   76.32   78.46   38.54   11.86   -2.95   -2.75"
      }
     ], 
     "prompt_number": 20
    }, 
    {
     "cell_type": "markdown", 
     "source": "**Summarize and discuss your results here**"
    }, 
    {
     "cell_type": "markdown", 
     "source": ""
    }
   ]
  }
 ]
}